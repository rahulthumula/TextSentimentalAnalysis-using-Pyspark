# -*- coding: utf-8 -*-
"""Sentimental_Analysis_on_Twitter_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12nBFWPY505_xI51AlB01X_V4Tml-X2Qo
"""

pip install findspark

pip install pyspark

import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
cols = ['sentiment','id','date','query_string','user','text']
df = pd.read_csv('training.1600000.processed.noemoticon.csv',header=None, names=cols,encoding='latin-1')
df.head()

df.sentiment.value_counts()

df.drop(['id','date','query_string','user'],axis=1,inplace=True)

df[df.sentiment == 0].head(10)

df[df.sentiment == 4].head(10)

df['pre_clean_len'] = [len(t) for t in df.text]

from pprint import pprint
data_dict = {
    'sentiment':{
        'type':df.sentiment.dtype,
        'description':'sentiment class - 0:negative, 1:positive'
    },
    'text':{
        'type':df.text.dtype,
        'description':'tweet text'
    },
    'pre_clean_len':{
        'type':df.pre_clean_len.dtype,
        'description':'Length of the tweet before cleaning'
    },
    'dataset_shape':df.shape
}
pprint(data_dict)

fig, ax = plt.subplots(figsize=(5, 5))
plt.boxplot(df.pre_clean_len)
plt.show()

df[df.pre_clean_len > 140].head(10)

df.text[279]

df.text[343]

import re
re.sub(r'@[A-Za-z0-9]+','',df.text[343])

df.text[0]

re.sub('https?://[A-Za-z0-9./]+','',df.text[0])

df.text[226]

testing = df.text[226]
testing

testing.replace(u"ï¿½", "?")

df.text[175]

re.sub("[^a-zA-Z]", " ", df.text[175])

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
import re
from bs4 import BeautifulSoup
from nltk.tokenize import WordPunctTokenizer
tok = WordPunctTokenizer()

pat1 = r'@[A-Za-z0-9_]+'
pat2 = r'https?://[^ ]+'
combined_pat = r'|'.join((pat1, pat2))
www_pat = r'www.[^ ]+'
negations_dic = {"isn't":"is not", "aren't":"are not", "wasn't":"was not", "weren't":"were not",
                "haven't":"have not","hasn't":"has not","hadn't":"had not","won't":"will not",
                "wouldn't":"would not", "don't":"do not", "doesn't":"does not","didn't":"did not",
                "can't":"can not","couldn't":"could not","shouldn't":"should not","mightn't":"might not",
                "mustn't":"must not"}
neg_pattern = re.compile(r'\b(' + '|'.join(negations_dic.keys()) + r')\b')

def tweet_cleaner_updated(text):
    soup = BeautifulSoup(text, 'lxml')
    souped = soup.get_text()
    try:
        bom_removed = souped.decode("utf-8-sig").replace(u"\ufffd", "?")
    except:
        bom_removed = souped
    stripped = re.sub(combined_pat, '', bom_removed)
    stripped = re.sub(www_pat, '', stripped)
    lower_case = stripped.lower()
    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)
    letters_only = re.sub("[^a-zA-Z]", " ", neg_handled)
    # During the letters_only process two lines above, it has created unnecessay white spaces,
    # I will tokenize and join together to remove unneccessary white spaces
    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]
    return (" ".join(words)).strip()

df = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',header=None,
                 usecols=[0,5],names=['sentiment','text'],encoding='latin-1')
df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})
df.head()

print ("Cleaning the tweets...\n")
clean_tweet_texts = []
for i in range(0,len(df)):
    if( (i+1)%100000 == 0 ):
        print ("Tweets %d of %d has been processed" % ( i+1, len(df) ) )                                                                  
    clean_tweet_texts.append(tweet_cleaner_updated(df['text'][i]))

clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])
clean_df['target'] = df.sentiment
clean_df.to_csv('clean_tweet.csv',encoding='utf-8')

clean_df.to_csv('clean_tweet.csv',encoding='utf-8')
csv = 'clean_tweet.csv'
my_df = pd.read_csv(csv,index_col=0)
my_df.head()

my_df.dropna(inplace=True)
my_df.reset_index(drop=True,inplace=True)
my_df.info()

neg_tweets = my_df[my_df.target == 0]
neg_string = []
for t in neg_tweets.text:
    neg_string.append(t)
neg_string = pd.Series(neg_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)
plt.figure(figsize=(12,10))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

pos_tweets = my_df[my_df.target == 1]
pos_string = []
for t in pos_tweets.text:
    pos_string.append(t)
pos_string = pd.Series(pos_string).str.cat(sep=' ')
wordcloud = WordCloud(width=1600, height=800,max_font_size=200,colormap='magma').generate(pos_string)
plt.figure(figsize=(12,10)) 
plt.imshow(wordcloud, interpolation="bilinear") 
plt.axis("off") 
plt.show()

"""Implementing Sentiment Analysis with Pyspark on clean_tweet.csv"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
        .appName("myApp") \
        .config("spark.driver.memory", "10g") \
        .getOrCreate()

sc = spark.sparkContext
df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('clean_tweet.csv')

type(df)

df.show(5)

df = df.dropna()

df.count()

(train_set, test_set) = df.randomSplit([0.90, 0.1], seed = 2000)

from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

"""HashingTF + IDF + Logistic Regression"""

from pyspark.ml.feature import HashingTF, IDF, Tokenizer
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline
from pyspark.ml.linalg import Vector

# Train the model using the entire dataset
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashtf = HashingTF(numFeatures=2**16, inputCol="words", outputCol='tf')
idf = IDF(inputCol='tf', outputCol="features", minDocFreq=5)
label_stringIdx = StringIndexer(inputCol = "target", outputCol = "label")
pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])
pipelineFit = pipeline.fit(train_set)
train_df = pipelineFit.transform(train_set)
test_df = pipelineFit.transform(test_set)

# Train a logistic regression model
from pyspark.ml.classification import LogisticRegression
lr1 = LogisticRegression(maxIter=100)
lrModel1 = lr1.fit(train_df)

from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics

# Compute predictions and convert to RDD
predictions = lrModel1.transform(test_df)
predictions_rdd = predictions.select(['prediction', 'label']).rdd

# Compute binary classification evaluation metrics
evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')
auc = evaluator.evaluate(predictions)

# Compute multiclass classification evaluation metrics
evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')
f1_score = evaluator.evaluate(predictions)

# Compute confusion matrix
metrics = MulticlassMetrics(predictions_rdd)
confusion_matrix = metrics.confusionMatrix().toArray()

print('F1 Score: {:.4f}'.format(f1_score))
print('Area Under the Curve (AUC): {:.4f}'.format(auc))
print('Confusion Matrix:')
print(confusion_matrix)
accuracy = evaluator.evaluate(predictions)
print("Accuracy:", accuracy)

# Create a function to predict the sentiment of a new sentence
def predict_sentiment(sentence: str) -> str:
    # Create a dataframe with the new sentence
    new_df = spark.createDataFrame([(sentence,)], ["text"])
    
    # Convert the text input into a feature vector
    new_df = pipelineFit.transform(new_df)
    features = new_df.select("features").head()[0]
    
    # Predict the sentiment of the new sentence
    prediction = lrModel1.predict(features)
    
    # Convert the prediction into a readable sentiment label
    return "Positive" if prediction == 1.0 else "Negative"

# Test the function
print(predict_sentiment("I love this product!")) # should print "Positive"
print(predict_sentiment("Rahul is a sad today")) # should print "Negative"

"""CountVectorizer + IDF + Logistic Regression"""

from pyspark.ml.feature import CountVectorizer

tokenizer = Tokenizer(inputCol="text", outputCol="words")
cv = CountVectorizer(vocabSize=2**16, inputCol="words", outputCol='cv')
idf = IDF(inputCol='cv', outputCol="features", minDocFreq=5) #minDocFreq: remove sparse terms
label_stringIdx = StringIndexer(inputCol = "target", outputCol = "label")
lr = LogisticRegression(maxIter=100)
pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])
pipelineFit = pipeline.fit(train_set)
predictions = pipelineFit.transform(test_set)
accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(test_set.count())
roc_auc = evaluator.evaluate(predictions)
print (accuracy)
print (roc_auc)

"""N-gram Implementation"""

from pyspark.ml.feature import NGram, VectorAssembler

def build_ngrams_wocs(inputCol=["text","target"], n=3):
    tokenizer = [Tokenizer(inputCol="text", outputCol="words")]
    ngrams = [
        NGram(n=i, inputCol="words", outputCol="{0}_grams".format(i))
        for i in range(1, n + 1)
    ]

    cv = [
        CountVectorizer(vocabSize=5460,inputCol="{0}_grams".format(i),
            outputCol="{0}_tf".format(i))
        for i in range(1, n + 1)
    ]
    idf = [IDF(inputCol="{0}_tf".format(i), outputCol="{0}_tfidf".format(i), minDocFreq=5) for i in range(1, n + 1)]

    assembler = [VectorAssembler(
        inputCols=["{0}_tfidf".format(i) for i in range(1, n + 1)],
        outputCol="features"
    )]
    label_stringIdx = [StringIndexer(inputCol = "target", outputCol = "label")]
    lr = [LogisticRegression(maxIter=100)]
    return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)

trigramwocs_pipelineFit = build_ngrams_wocs().fit(train_set)
predictions_wocs = trigramwocs_pipelineFit.transform(test_set)
accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float(test_set.count())
roc_auc_wocs = evaluator.evaluate(predictions_wocs)
print (accuracy_wocs)
print (roc_auc_wocs)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics

# Compute F1 score
evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')
f1_score_wocs = evaluator.evaluate(predictions_wocs)
print('F1 Score (w/ Trigram and WOCS):', f1_score_wocs)

# Compute confusion matrix
predictions_rdd = predictions_wocs.select(['prediction', 'label']).rdd
metrics = MulticlassMetrics(predictions_rdd)
confusion_matrix_wocs = metrics.confusionMatrix().toArray()
print('Confusion Matrix (w/ Trigram and WOCS):\n', confusion_matrix_wocs)

"""### Test Cases"""

#@title Default title text
# Sample test sentences
test_sentences = [
    "This movie was fantastic, I loved it!",
    "The acting was terrible and the plot was nonsensical.",
    "The special effects were incredible but the pacing was too slow.",
    "it was a waste of time and money.",
    "The story was engaging and the characters were well-developed.",
]

# Create DataFrame from test sentences
test_df = spark.createDataFrame([(s,) for s in test_sentences], ["text"])

# Apply the trained pipeline to the test DataFrame
predictions = trigramwocs_pipelineFit.transform(test_df)


predictions.select("text", "prediction", "probability").show(truncate=False)